{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLgU__PsfHva"
      },
      "source": [
        "# **Chatbox with Zephyr 7B**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDxdhdwbfSlv"
      },
      "source": [
        "We will develop a chatbot using **Zephyr 7B**, a well-regarded conversational model that has been enhanced through fine-tuning with Mistral 7B. Crafted by the Hugging Face H4 team, Zephyr 7B Beta has demonstrated superior performance compared to GPT-3.5, LLama 2 70B, and other models in the 7B size category, as evidenced by the Alpaca Leaderboard rankings. Additionally, Zephyr 7B offers the advantage of being significantly more compact, requiring only 4% of the storage space needed for GPT-3.5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYe7FrmogwSl"
      },
      "source": [
        "# **Load Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpF2Hc0e7OHw"
      },
      "outputs": [],
      "source": [
        "# You only need to run this once, even if you re-start the kernel or machine\n",
        "# Remove the -q to print install output\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U accelerate jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv4zVP3JhGbF"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoYTQmR2hMSK"
      },
      "source": [
        "In this section, we initialize the model. You're welcome to substitute it with any conversational or instructional model from Hugging Face, ensuring that you adjust the \"roles\" accordingly as outlined below. However, this model is for now highly recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q9nSO3l7v26"
      },
      "outputs": [],
      "source": [
        "models_dict = {\n",
        "    \"Zephyr 7B Alpha\": \"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    \"Zephyr 7B Beta\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    # You can add more chat/instruct models here\n",
        "}\n",
        "\n",
        "model_id = models_dict[\"Zephyr 7B Beta\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8KOlOXw70-C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Running this may take a minute\n",
        "pipe = pipeline(\"text-generation\", model=model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl5in3i8htJb"
      },
      "source": [
        "# **Define the role**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQsemPbh0Qp"
      },
      "source": [
        "specify the role you want the chatbot to adopt. For this example, I'll configure it to function as a sports analyst."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2SBgQC_8G16"
      },
      "outputs": [],
      "source": [
        "job_description = \"You are a sport analyst expert. Please answer my questions.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27fjsHrpiqqT"
      },
      "source": [
        "Here we execute the model! To conclude the question-answering session, type \"stop\" or \"exit\". Feel free to disregard the UserWarning about changes to the pre-trained model configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apMCHExl8U07"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": job_description,\n",
        "    },\n",
        "]\n",
        "\n",
        "exit_terms = [\"stop\", \"exit\"]\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nQuestion:\")\n",
        "    if question.lower() in exit_terms:\n",
        "        break\n",
        "    messages.append({\"role\": \"user\", \"content\": question})\n",
        "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    output = outputs[0][\"generated_text\"]\n",
        "    messages.append({\"role\": \"assistant\", \"content\": output})\n",
        "    response_start = output.rfind('<|assistant|>')\n",
        "    print(output[response_start + len('<|assistant|>'):])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
